# -*- coding: utf-8 -*-
"""ingest_eca_products.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14cwcUJxSPmLv4WaTyjOUEqGOFoQVbG7G
"""

!git clone https://github.com/SupremeTech-ir/ELECyar.git

# Commented out IPython magic to ensure Python compatibility.
# %cd ELECyar

# !pip install langchain langchain-community chromadb sentence-transformers

!pip install -U \
  langchain==0.1.16 \
  langchain-community==0.0.32 \
  chromadb==0.4.24 \
  sentence-transformers==2.6.1 \
  opentelemetry-api==1.22.0 \
  opentelemetry-sdk==1.22.0 \
  opentelemetry-exporter-otlp==1.22.0 \
  requests==2.31.0

import langchain
import chromadb
from sentence_transformers import SentenceTransformer

print("LangChain:", langchain.__version__)
print("ChromaDB:", chromadb.__version__)
print("SentenceTransformer OK")

import os

os.listdir(".")

os.listdir("ELECyar")

os.listdir("ELECyar/eca_products_merged")

from langchain.schema import Document
import os

BASE_DIR = "ELECyar/eca_products_merged"

documents = []

for category in os.listdir(BASE_DIR):
    category_path = os.path.join(BASE_DIR, category)

    if not os.path.isdir(category_path):
        continue

    for filename in os.listdir(category_path):
        if filename.endswith(".txt"):
            file_path = os.path.join(category_path, filename)

            with open(file_path, "r", encoding="utf-8") as f:
                text = f.read()

            documents.append(
                Document(
                    page_content=text,
                    metadata={
                        "category": category,
                        "source": filename}))

print("Loaded documents:", len(documents))
print("Sample doc metadata:", documents[0].metadata)
print("Sample text preview:", documents[0].page_content[:300])

!pip install langchain-text-splitters

# from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_text_splitters import RecursiveCharacterTextSplitter


text_splitter = RecursiveCharacterTextSplitter(chunk_size=800,chunk_overlap=100)

chunks = text_splitter.split_documents(documents)

print("Total chunks:", len(chunks))
print("Sample chunk metadata:", chunks[0].metadata)
print("Sample chunk preview:", chunks[0].page_content[:300])

from langchain_community.embeddings import HuggingFaceEmbeddings

embedding = HuggingFaceEmbeddings(
    model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

print("Embedding model loaded")

vec = embedding.embed_query("دیود پل 10 آمپر 1000 ولت")
print(len(vec))

from langchain_community.vectorstores import Chroma

test_chunks = chunks[:20]

vectorstore = Chroma.from_documents(
    documents=test_chunks,
    embedding=embedding,
    persist_directory="chroma_test_db"
)

print("Test vector store created")

results = vectorstore.similarity_search("دیود پل 10 آمپر 1000 ولت",k=3)

for r in results:
    print(r.metadata)
    print(r.page_content[:300])
    print("-" * 40)

from langchain_community.vectorstores import Chroma

vectorstore = Chroma.from_documents(documents=chunks,embedding=embedding,persist_directory="eca_products_vector_db")
vectorstore.persist()

print("Final Vector DB created and persisted")