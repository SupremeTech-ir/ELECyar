import asyncio
import os
import random
import re
from playwright.async_api import async_playwright
from urllib.parse import urljoin, urlparse

# Configuration - Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ±
BASE_URL = "https://eshop.eca.ir"
OUTPUT_DIR = "scraped_data"
MIN_DELAY = 3  # Ø­Ø¯Ø§Ù‚Ù„ ÙˆÙ‚ÙÙ‡ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)
MAX_DELAY = 8  # Ø­Ø¯Ø§Ú©Ø«Ø± ÙˆÙ‚ÙÙ‡ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ (Ø«Ø§Ù†ÛŒÙ‡)
MAX_PAGES = None  # ØªØ¹Ø¯Ø§Ø¯ ØµÙØ­Ø§Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± (None = Ù‡Ù…Ù‡ ØµÙØ­Ø§Øª)
SCRAPED_URLS_FILE = "scraped_urls.txt"  # ÙØ§ÛŒÙ„ Ø°Ø®ÛŒØ±Ù‡ URLÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡

# Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§ Ùˆ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ù…Ø±ØªØ¨Ø·
CATEGORIES = {
    "Ù…Ù‚Ø§ÙˆÙ…Øª": ["Ù…Ù‚Ø§ÙˆÙ…Øª", "resistor"],
    "Ø®Ø§Ø²Ù†": ["Ø®Ø§Ø²Ù†", "capacitor"],
    "Ø³Ù„Ù": ["Ø³Ù„Ù", "inductor", "coil"],
    "Ø¯ÛŒÙˆØ¯": ["Ø¯ÛŒÙˆØ¯", "diode"],
    "Ø¢ÛŒ Ø³ÛŒ - ØªØ±Ø§Ø´Ù‡": ["Ø¢ÛŒ Ø³ÛŒ", "ØªØ±Ø§Ø´Ù‡", "ic", "chip"],
    "Ù…ÛŒÚ©Ø±ÙˆÚ©Ù†ØªØ±Ù„Ø± Ùˆ Ù¾Ø±ÙˆØ³Ø³ÙˆØ±": ["Ù…ÛŒÚ©Ø±ÙˆÚ©Ù†ØªØ±Ù„Ø±", "Ù…ÛŒÚ©Ø±Ùˆ Ú©Ù†ØªØ±Ù„Ø±", "Ù¾Ø±ÙˆØ³Ø³ÙˆØ±", "microcontroller", "processor", "mcu"],
    "Ø±Ú¯ÙˆÙ„Ø§ØªÙˆØ±": ["Ø±Ú¯ÙˆÙ„Ø§ØªÙˆØ±", "regulator"],
    "ØªØ±Ø§Ù†Ø²ÛŒØ³ØªÙˆØ±": ["ØªØ±Ø§Ù†Ø²ÛŒØ³ØªÙˆØ±", "transistor"],
    "ØªØ±Ø§ÛŒØ§Ú© Ùˆ ØªØ±ÛŒØ³ØªÙˆØ±": ["ØªØ±Ø§ÛŒØ§Ú©", "ØªØ±ÛŒØ³ØªÙˆØ±", "triac", "thyristor"],
    "LED Ùˆ ØªØ¬Ù‡ÛŒØ²Ø§Øª Ù…Ø±ØªØ¨Ø·": ["led", "Ø§Ù„ Ø§ÛŒ Ø¯ÛŒ"],
    "Ø³Ú¯Ù…Ù†Øª Ùˆ Ù…Ø§ØªØ±ÛŒØ³": ["Ø³Ú¯Ù…Ù†Øª", "Ù…Ø§ØªØ±ÛŒØ³", "segment", "matrix"],
    "Ú©Ø±ÛŒØ³ØªØ§Ù„ Ùˆ Ø§Ø³ÛŒÙ„Ø§ØªÙˆØ±": ["Ú©Ø±ÛŒØ³ØªØ§Ù„", "Ø§Ø³ÛŒÙ„Ø§ØªÙˆØ±", "crystal", "oscillator"],
    "ÙˆØ±ÛŒØ³ØªÙˆØ±": ["ÙˆØ±ÛŒØ³ØªÙˆØ±", "varistor"],
    "Ø±Ù„Ù‡": ["Ø±Ù„Ù‡", "relay"],
    "Ù¾ÛŒÙ† Ù‡Ø¯Ø±": ["Ù¾ÛŒÙ† Ù‡Ø¯Ø±", "pin header", "header"],
    "Ø³ÙˆÙƒØªØŒ Ú©Ø§Ù†Ú©ØªÙˆØ±ØŒ ÙÛŒØ´": ["Ø³ÙˆÚ©Øª", "Ú©Ø§Ù†Ú©ØªÙˆØ±", "ÙÛŒØ´", "socket", "connector"],
    "Ú©Ù„ÛŒØ¯ØŒ Ø³ÙˆØ¦ÛŒÚ†ØŒ Ú©ÛŒÙ¾Ø¯": ["Ú©Ù„ÛŒØ¯", "Ø³ÙˆØ¦ÛŒÚ†", "Ú©ÛŒÙ¾Ø¯", "switch", "keypad", "button"],
    "ØªØ±Ù…ÛŒÙ†Ø§Ù„ Ù¾ÛŒÚ†ÛŒ Ùˆ Ú©Ø´ÙˆÛŒÛŒ": ["ØªØ±Ù…ÛŒÙ†Ø§Ù„", "terminal"],
    "ÙÛŒÙˆØ²": ["ÙÛŒÙˆØ²", "fuse"],
    "Ø¨Ø§Ø²Ø±ØŒ Ù¾ÛŒØ²Ùˆ Ùˆ Ø¨Ù„Ù†Ø¯Ú¯Ùˆ": ["Ø¨Ø§Ø²Ø±", "Ù¾ÛŒØ²Ùˆ", "Ø¨Ù„Ù†Ø¯Ú¯Ùˆ", "buzzer", "piezo", "speaker"],
    "Ø¢Ù†ØªÙ†": ["Ø¢Ù†ØªÙ†", "antenna"],
    "Ø±ÛŒÙ…ÙˆØª Ú©Ù†ØªØ±Ù„Ø±": ["Ø±ÛŒÙ…ÙˆØª", "remote"],
    "ÙÛŒØ¨Ø± Ù…Ø¯Ø§Ø± Ú†Ø§Ù¾ÛŒ - Ø¨Ø±Ø¯ Ø¨ÙˆØ±Ø¯": ["ÙÛŒØ¨Ø±", "Ø¨Ø±Ø¯", "pcb", "breadboard", "Ø¨ÙˆØ±Ø¯"],
    "Ø³ÛŒÙ… Ùˆ Ú©Ø§Ø¨Ù„": ["Ø³ÛŒÙ…", "Ú©Ø§Ø¨Ù„", "wire", "cable"],
    "ØªØ±Ø§Ù†Ø³ØŒ Ú†ÙˆÚ©ØŒ ÙØ±ÛŒØªØŒ Ù‡Ø³ØªÙ‡": ["ØªØ±Ø§Ù†Ø³", "Ú†ÙˆÚ©", "ÙØ±ÛŒØª", "Ù‡Ø³ØªÙ‡", "transformer", "choke", "ferrite", "core"],
    "Ù¾ÙˆÚ¯Ùˆ Ù¾ÛŒÙ† - Ù¾ÛŒÙ† ØªØ³Øª": ["Ù¾ÙˆÚ¯Ùˆ", "Ù¾ÛŒÙ† ØªØ³Øª", "pogo", "test pin"],
    "ÙÙ† Ùˆ Ù…Ø­Ø§ÙØ¸ ÙÙ†": ["ÙÙ†", "fan"],
    "Ù‡ÛŒØª Ø³ÛŒÙ†Ú© Ùˆ Ø§Ù„Ù…Ø§Ù† Ø­Ø±Ø§Ø±ØªÛŒ": ["Ù‡ÛŒØª Ø³ÛŒÙ†Ú©", "Ø§Ù„Ù…Ø§Ù† Ø­Ø±Ø§Ø±ØªÛŒ", "heat sink", "heatsink"],
    "Ù„ÛŒØ²Ø±": ["Ù„ÛŒØ²Ø±", "laser"],
    "Ø§Ø³Ù¾Ø§Ø±Ú© Ú¯Ù¾": ["Ø§Ø³Ù¾Ø§Ø±Ú© Ú¯Ù¾", "spark gap"],
    "Ù¾ÛŒÚ† Ùˆ Ø§Ø³Ù¾ÛŒØ³Ø±": ["Ù¾ÛŒÚ†", "Ø§Ø³Ù¾ÛŒØ³Ø±", "screw", "spacer"],
    "Ø¬Ø¹Ø¨Ù‡ Ùˆ Ú©ÛŒØ³ Ø¨Ø±Ø¯Ù‡Ø§ÛŒ Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ©ÛŒ": ["Ø¬Ø¹Ø¨Ù‡", "Ú©ÛŒØ³", "box", "case", "enclosure"],
    "Ø¨Ø±Ù‚ Ø³Ø§Ø®ØªÙ…Ø§Ù†": ["Ø¨Ø±Ù‚ Ø³Ø§Ø®ØªÙ…Ø§Ù†", "Ù¾Ø±ÛŒØ²", "Ú©Ù„ÛŒØ¯ Ø¨Ø±Ù‚"],
    "Ù…ØªÙØ±Ù‚Ù‡": []  # Ø¨Ø±Ø§ÛŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¨Ø¯ÙˆÙ† Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
}

visited_urls = set()
scraped_count = 0
category_counts = {cat: 0 for cat in CATEGORIES.keys()}

def load_scraped_urls(filepath: str) -> set:
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª URLÙ‡Ø§ÛŒ Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡"""
    if os.path.exists(filepath):
        with open(filepath, "r", encoding="utf-8") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

def save_scraped_url(filepath: str, url: str):
    """Ø°Ø®ÛŒØ±Ù‡ URL Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„"""
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(f"{url}\n")

def detect_category(title: str) -> str:
    """ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ù†ÙˆØ§Ù†"""
    text = title.lower()
    
    for category, keywords in CATEGORIES.items():
        if category == "Ù…ØªÙØ±Ù‚Ù‡":
            continue
        for keyword in keywords:
            if keyword.lower() in text:
                return category
    
    return "Ù…ØªÙØ±Ù‚Ù‡"

async def extract_product_info(page, url: str):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø­ØµÙˆÙ„ Ø§Ø² ÛŒÚ© ØµÙØ­Ù‡"""
    result = f"URL: {url}\n{'='*80}\n\n"
    
    title = ""
    # 1. Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ Ù…Ø­ØµÙˆÙ„
    try:
        title_element = await page.query_selector("h1")
        if title_element:
            title = await title_element.inner_text()
            result += f"=== Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ ===\n{title.strip()}\n\n"
    except:
        result += "=== Ø¹Ù†ÙˆØ§Ù† Ø§ØµÙ„ÛŒ ===\nÛŒØ§ÙØª Ù†Ø´Ø¯\n\n"
    
    # 2. Ø¹Ù†ÙˆØ§Ù† Ø¯ÙˆÙ…/Ø²ÛŒØ±Ø¹Ù†ÙˆØ§Ù†
    try:
        subtitle_element = await page.query_selector("[class*='desc']")
        if subtitle_element:
            subtitle = await subtitle_element.inner_text()
            result += f"=== Ø¹Ù†ÙˆØ§Ù† Ø¯ÙˆÙ… ===\n{subtitle.strip()}\n\n"
    except:
        result += "=== Ø¹Ù†ÙˆØ§Ù† Ø¯ÙˆÙ… ===\nÛŒØ§ÙØª Ù†Ø´Ø¯\n\n"
    
    # 3. ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„
    try:
        description_element = await page.query_selector(".product-description")
        if description_element:
            description = await description_element.inner_text()
            result += f"=== ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„ ===\n{description.strip()}\n\n"
    except:
        result += "=== ØªÙˆØ¶ÛŒØ­Ø§Øª Ú©Ø§Ù…Ù„ ===\nÛŒØ§ÙØª Ù†Ø´Ø¯\n\n"
    
    # 4. Ù‚ÛŒÙ…Øª
    try:
        price_element = await page.query_selector("span.current-price.fa-number-conv")
        if price_element:
            price = await price_element.inner_text()
            result += f"=== Ù‚ÛŒÙ…Øª ===\n{price.strip()}\n\n"
    except:
        result += "=== Ù‚ÛŒÙ…Øª ===\nÛŒØ§ÙØª Ù†Ø´Ø¯\n\n"
    
    return result, title

async def get_all_links(page, base_url: str):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙ…Ø§Ù… Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø² ØµÙØ­Ù‡"""
    links = set()
    try:
        all_links = await page.query_selector_all("a[href]")
        for link in all_links:
            href = await link.get_attribute("href")
            if href:
                full_url = urljoin(base_url, href)
                parsed = urlparse(full_url)
                if parsed.netloc == urlparse(base_url).netloc:
                    links.add(full_url)
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§: {e}")
    
    return links

async def scrape_page(page, url: str, output_dir: str, scraped_urls_file: str):
    """Ø§Ø³Ú©Ø±ÙÛŒÙ¾ ÛŒÚ© ØµÙØ­Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª"""
    global scraped_count, category_counts
    
    try:
        print(f"Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³Ú©Ø±ÙÛŒÙ¾: {url}")
        await page.goto(url, wait_until="networkidle", timeout=30000)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø·Ù„Ø§Ø¹Ø§Øª
        content, title = await extract_product_info(page, url)
        
        # ØªØ´Ø®ÛŒØµ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        category = detect_category(title)
        
        # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
        category_dir = os.path.join(output_dir, category)
        os.makedirs(category_dir, exist_ok=True)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„
        filename = f"page_{category_counts[category]:04d}.txt"
        filepath = os.path.join(category_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        
        # Ø°Ø®ÛŒØ±Ù‡ URL Ø¯Ø± ÙØ§ÛŒÙ„ URLÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡
        save_scraped_url(scraped_urls_file, url)
        
        category_counts[category] += 1
        scraped_count += 1
        print(f"âœ“ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯ Ø¯Ø± [{category}]: {filename} (ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„: {scraped_count})")
        
        return True
    except Exception as e:
        print(f"âœ— Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±ÙÛŒÙ¾ {url}: {e}")
        return False

async def scrape_domain(base_url: str, output_dir: str, min_delay: int, max_delay: int, max_pages: int = None):
    """Ø§Ø³Ú©Ø±ÙÛŒÙ¾ Ú©Ù„ Ø¯Ø§Ù…Ù†Ù‡ Ø¨Ø§ ÙˆÙ‚ÙÙ‡ ØªØµØ§Ø¯ÙÛŒ"""
    global visited_urls, scraped_count
    
    # Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¯Ø± Ú©Ù†Ø§Ø± ÙØ§ÛŒÙ„ Ú©Ø¯
    script_dir = os.path.dirname(os.path.abspath(__file__))
    full_output_dir = os.path.join(script_dir, output_dir)
    os.makedirs(full_output_dir, exist_ok=True)
    
    # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ URLÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡
    scraped_urls_file = os.path.join(script_dir, SCRAPED_URLS_FILE)
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ URLÙ‡Ø§ÛŒ Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡
    already_scraped = load_scraped_urls(scraped_urls_file)
    visited_urls.update(already_scraped)
    
    if already_scraped:
        print(f"ğŸ”„ ØªØ¹Ø¯Ø§Ø¯ {len(already_scraped)} URL Ù‚Ø¨Ù„Ø§Ù‹ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡ Ø§Ø³Øª Ùˆ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n")
    
    async with async_playwright() as pw:
        browser = await pw.chromium.launch(headless=True)
        page = await browser.new_page()
        
        # ØµÙ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
        to_visit = {base_url}
        
        while to_visit and (max_pages is None or scraped_count < max_pages):
            # Ø§Ù†ØªØ®Ø§Ø¨ Ù„ÛŒÙ†Ú© Ø¨Ø¹Ø¯ÛŒ
            current_url = to_visit.pop()
            
            if current_url in visited_urls:
                continue
            
            visited_urls.add(current_url)
            
            # Ø§Ø³Ú©Ø±ÙÛŒÙ¾ ØµÙØ­Ù‡
            success = await scrape_page(page, current_url, full_output_dir, scraped_urls_file)
            
            if success:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                new_links = await get_all_links(page, base_url)
                to_visit.update(new_links - visited_urls)
                
                # ÙˆÙ‚ÙÙ‡ ØªØµØ§Ø¯ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø±ÙØªØ§Ø± Ø§Ù†Ø³Ø§Ù†ÛŒ
                delay = random.uniform(min_delay, max_delay)
                print(f"â³ ÙˆÙ‚ÙÙ‡ {delay:.2f} Ø«Ø§Ù†ÛŒÙ‡...\n")
                await asyncio.sleep(delay)
        
        await browser.close()
        
        print(f"\n{'='*80}")
        print(f"Ø§Ø³Ú©Ø±ÙÛŒÙ¾ Ú©Ø§Ù…Ù„ Ø´Ø¯!")
        print(f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ ØµÙØ­Ø§Øª Ø§Ø³Ú©Ø±ÙÛŒÙ¾ Ø´Ø¯Ù‡: {scraped_count}")
        print(f"\nØªÙˆØ²ÛŒØ¹ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒâ€ŒÙ‡Ø§:")
        for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):
            if count > 0:
                print(f"  {category}: {count} Ù…Ø­ØµÙˆÙ„")
        print(f"\nÙ…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡: {full_output_dir}")
        print(f"ÙØ§ÛŒÙ„ URLÙ‡Ø§ÛŒ Ø§Ø³Ú©Ø±Ù¾ Ø´Ø¯Ù‡: {scraped_urls_file}")
        print(f"{'='*80}")

if __name__ == "__main__":
    asyncio.run(scrape_domain(BASE_URL, OUTPUT_DIR, MIN_DELAY, MAX_DELAY, MAX_PAGES))
